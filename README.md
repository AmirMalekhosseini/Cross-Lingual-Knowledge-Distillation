# Cross-Lingual-Knowledge-Distillation
 Designed a pipeline using CLIP-style contrastive loss to distill knowledge from a large English teacher model into a smaller multilingual student model, aligning their cross-lingual sentence embeddings.
